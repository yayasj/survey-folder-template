# Survey Configuration
# This file contains all survey-specific settings for the data pipeline

# Project Information
project:
  name: "{{cookiecutter.project_name}}"
  slug: "{{cookiecutter.project_slug}}"
  description: "{{cookiecutter.project_description}}"
  client: "{{cookiecutter.client_name}}"
  start_date: "{{cookiecutter.survey_start_date}}"
  end_date: "{{cookiecutter.survey_end_date}}"

# Team Information
team:
  data_manager:
    name: "{{cookiecutter.data_manager_name}}"
    email: "{{cookiecutter.data_manager_email}}"
  qa_officer:
    name: "{{cookiecutter.qa_officer_name}}"
    email: "{{cookiecutter.qa_officer_email}}"

# ODK Central Configuration
odk:
  base_url: "{{cookiecutter.odk_central_url}}"
  # Credentials loaded from environment variables:
  username: "${ODK_USERNAME}"
  password: "${ODK_PASSWORD}"
  project_id: "${ODK_PROJECT_ID}"
  
  # List of forms to download (will be auto-discovered if empty)
  forms: []
  
  # Download format options: csv, json
  download_format: "csv"
  
  # Include media attachments
  include_attachments: false
  
  # Data processing options for handling ODK export issues
  clean_column_headers: true      # Fix unnamed columns and group header issues
  remove_empty_columns: true      # Remove completely empty columns from exports
  use_fallback_export: true       # Use alternative export method if primary fails
  flatten_group_headers: true     # Use ODK Central CSV API with flattened group headers (removes meta/ prefixes)

# Pipeline configuration
staging:
  raw_data_dir: "staging/raw"
  cleaned_data_dir: "staging/cleaned"
  failed_data_dir: "staging/failed"
  
data_formats:
  date_format: "%Y-%m-%d"
  datetime_format: "%Y-%m-%d %H:%M:%S"
  
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Validation configuration
validation:
  minimum_pass_rate: 85.0  # Minimum % of expectations that must pass
  fail_fast_on_critical: true  # Stop pipeline on critical validation failures
  save_failed_rows: true  # Extract failed rows to separate files
  abort_on_structural_failure: true
  continue_on_business_logic_failure: true
  max_iterations: 5
  create_backup: true
  rules_file: "cleaning_rules.xlsx"
  
# Dataset configurations with validation suites
datasets:
  household_dataset:
    file_pattern: "household*.csv"
    validation_suite: "household_suite"
    cleaning_rules: "household_cleaning_rules.xlsx"
    required: true
    primary_key: ["household_id", "submission_uuid"]
    
  individual_dataset:
    file_pattern: "individual*.csv"
    validation_suite: "individual_suite"
    cleaning_rules: "individual_cleaning_rules.xlsx"
    required: true
    primary_key: ["respondent_id"]

# Administrative columns to include in failed row extracts
admin_columns:
  - "instanceID"
  - "SubmitterName"

# Publishing settings
publish:
  stable_directory: "cleaned_stable"
  backup_previous: true
  notify_on_success: true
  notify_on_failure: true

# Streamlit dashboard settings
dashboard:
  port: 8501  # Default port, override with STREAMLIT_SERVER_PORT
  title: "{{cookiecutter.project_name}} - Data Dashboard"
  refresh_interval: 300  # seconds
  logo_path: "assets/logo.png"
  features:
    submission_trends: true
    quality_metrics: true
    geographical_view: true
    enumerator_performance: true

# Prefect settings
prefect:
  flow_name: "{{cookiecutter.project_slug}}_pipeline"
  max_retries: 3
  retry_delay: 60  # seconds
  max_concurrent_tasks: 2

# Performance tuning
performance:
  chunk_size: 10000
  memory_limit: 4096
  n_workers: 2

# Log retention (days)
retention_days: 30
  
# Log file locations
log_files:
  ingestion: "logs/ingest_{date}.log"
  validation: "logs/validate_{date}.log" 
  cleaning: "logs/clean_{date}.log"
  publish: "logs/publish_{date}.log"
